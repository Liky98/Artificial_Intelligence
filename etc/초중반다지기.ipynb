{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"초중반다지기.ipynb","private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyPuGeUQLP0eHnwblEQr7XFX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"APtKmg5Ls_3g"},"outputs":[],"source":["import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","source":["en_text = \"A Dog Run back corner near spare bedrooms\"\n","\n","from nltk.tokenize import word_tokenize\n","print(word_tokenize(en_text))"],"metadata":{"id":"92utLyiLtDiv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n","%cd Mecab-ko-for-Google-Colab\n","!bash install_mecab-ko_on_colab190912.sh"],"metadata":{"id":"zgHLpcaotbm_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(list(en_text))"],"metadata":{"id":"He2kner_tPW1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import urllib.request\n","import pandas as pd\n","from konlpy.tag import Mecab\n","from nltk import FreqDist\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")\n","data = pd.read_table('ratings.txt') # 데이터프레임에 저장\n","data[:10]\n"],"metadata":{"id":"RKZy2OUAtPy-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_data = data[:100] # 임의로 100개만 저장\n","\n","sample_data['document'] = sample_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n","# 한글과 공백을 제외하고 모두 제거\n","sample_data[:10]\n","\n","# 불용어 정의\n","stopwords=['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n","\n","tokenizer = Mecab()\n","tokenized=[]\n","for sentence in sample_data['document']:\n","    temp = tokenizer.morphs(sentence) # 토큰화\n","    temp = [word for word in temp if not word in stopwords] # 불용어 제거\n","    tokenized.append(temp)\n","\n","print(tokenized[:10])\n"],"metadata":{"id":"EOOhzphovYB2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab = FreqDist(np.hstack(tokenized))\n","print('단어 집합의 크기 : {}'.format(len(vocab)))\n","\n","vocab_size = 500\n","# 상위 vocab_size개의 단어만 보존\n","vocab = vocab.most_common(vocab_size)\n","print('단어 집합의 크기 : {}'.format(len(vocab)))"],"metadata":{"id":"TfCAPG78vhhX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["word_to_index = {word[0] : index + 2 for index, word in enumerate(vocab)}\n","word_to_index['pad'] = 1\n","word_to_index['unk'] = 0\n","word_to_index"],"metadata":{"id":"4yqsQe1mvlSG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoded = []\n","for line in tokenized: #입력 데이터에서 1줄씩 문장을 읽음\n","    temp = []\n","    for w in line: #각 줄에서 1개씩 글자를 읽음\n","      try:\n","        temp.append(word_to_index[w]) # 글자를 해당되는 정수로 변환\n","      except KeyError: # 단어 집합에 없는 단어일 경우 unk로 대체된다.\n","        temp.append(word_to_index['unk']) # unk의 인덱스로 변환\n","\n","    encoded.append(temp)\n","\n","max_len = max(len(l) for l in encoded)\n","print('리뷰의 최대 길이 : %d' % max_len)\n","print('리뷰의 최소 길이 : %d' % min(len(l) for l in encoded))\n","print('리뷰의 평균 길이 : %f' % (sum(map(len, encoded))/len(encoded)))\n","plt.hist([len(s) for s in encoded], bins=50)\n","plt.xlabel('length of sample')\n","plt.ylabel('number of sample')\n","plt.show()"],"metadata":{"id":"m6Gam-PKwIOQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"TTW-al5VwInG"},"execution_count":null,"outputs":[]}]}