{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SGNS 구현.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPDwKjyOatBxtC4d2OZtJtX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"FqyBSjSNjxTl"},"outputs":[],"source":["\"\"\" \n","Skip-Gram with Negative Sampling, SGNS\n","\"\"\"\n","import pandas as pd\n","import numpy as np\n","import nltk\n","from nltk.corpus import stopwords\n","from sklearn.datasets import fetch_20newsgroups\n","from tensorflow.keras.preprocessing.text import Tokenizer\n"]},{"cell_type":"code","source":["\"\"\"20뉴스그룹 데이터를 사용합니다. \n","이번 실습에서는 하나의 샘플에 최소 단어 2개는 있어야 합니다. \n","그래야만 중심 단어, 주변 단어의 관계가 성립하며 그렇지 않으면 샘플을 구성할 수 없어 에러가 발생합니다. \n","전처리 과정에서 지속적으로 이를 만족하지 않는 샘플들을 제거하겠습니다.\n","\"\"\"\n","dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n","documents = dataset.data\n","print('총 샘플 수 :',len(documents))\n"],"metadata":{"id":"BbFvJxnoj_5C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["news_df = pd.DataFrame({'document':documents})\n","# 특수 문자 제거\n","news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n","# 길이가 3이하인 단어는 제거 (길이가 짧은 단어 제거)\n","news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n","# 전체 단어에 대한 소문자 변환\n","news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"],"metadata":{"id":"0VQUdavskJKY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# NULL 있는지 확인\n","news_df.isnull().values.any()\n","\n","#Null 값이 없지만, 빈 값(empy) 유무도 확인해야 합니다. \n","#모든 빈 값을 Null 값으로 변환하고, 다시 Null 값이 있는지 확인합니다.\n","news_df.replace(\"\", float(\"NaN\"), inplace=True)\n","print(news_df.isnull().values.any())\n","\n","news_df.dropna(inplace=True)\n","print('총 샘플 수 :',len(news_df))"],"metadata":{"id":"4PnYhF2XkRAJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nltk.download('stopwords')"],"metadata":{"id":"F-LBNsq_ltXf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# NLTK에서 정의한 불용어 리스트를 사용하여 불용어를 제거합니다.\n","stop_words = nltk.corpus.stopwords.words('english')\n","tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\n","tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n","tokenized_doc = tokenized_doc.to_list()"],"metadata":{"id":"EZxoHaLDkXiY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 단어가 1개 이하인 샘플의 인덱스를 찾아서 저장하고, 해당 샘플들은 제거.\n","drop_train = [index for index, sentence in enumerate(tokenized_doc) if len(sentence) <= 1]\n","tokenized_doc = np.delete(tokenized_doc, drop_train, axis=0)\n","print('총 샘플 수 :',len(tokenized_doc))"],"metadata":{"id":"4808O4T0lN2q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#단어 집합을 생성하고, 정수 인코딩을 진행합니다.\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(tokenized_doc)\n","\n","word2idx = tokenizer.word_index\n","idx2word = {value : key for key, value in word2idx.items()}\n","encoded = tokenizer.texts_to_sequences(tokenized_doc)"],"metadata":{"id":"sIaES2aSl0Io"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#상위 2개 샘플\n","print(encoded[:2])\n","\n","# 단어집합 크기\n","vocab_size = len(word2idx) + 1 \n","print('단어 집합의 크기 :', vocab_size)"],"metadata":{"id":"uLjHUFSUl3GA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["토큰화, 정제, 정규화, 불용어 제거, 정수 인코딩까지 일반적인 전처리 과정을 거쳤습니다. 네거티브 샘플링을 통한 데이터셋을 구성할 차례입니다. 이를 위해서는 네거티브 샘플링을 위해서 케라스에서 제공하는 전처리 도구인 skipgrams를 사용합니다. 어떤 전처리가 수행되는지 그 결과를 확인하기 위해서 (꽤 시간이 소요되는 작업이므로) 상위 10개의 뉴스그룹 샘플에 대해서만 수행해봅시다."],"metadata":{"id":"rgcS1R6_mF1H"}},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.sequence import skipgrams\n","# 네거티브 샘플링\n","skip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded[:10]]"],"metadata":{"id":"-kir-epIl6m6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#결과를 확인합니다. 10개의 뉴스그룹 샘플에 대해서 모두 수행되었지만, 첫번째 뉴스그룹 샘플에 대해서만 확인해보겠습니다.\n","# 첫번째 샘플인 skip_grams[0] 내 skipgrams로 형성된 데이터셋 확인\n","pairs, labels = skip_grams[0][0], skip_grams[0][1]\n","for i in range(5):\n","    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n","          idx2word[pairs[i][0]], pairs[i][0], \n","          idx2word[pairs[i][1]], pairs[i][1], \n","          labels[i]))"],"metadata":{"id":"WkB-rt7fmKEX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#윈도우 크기 내에서 중심 단어, 주변 단어의 관계를 가지는 경우에는 1의 레이블을 갖도록 하고, \n","#그렇지 않은 경우는 0의 레이블을 가지도록 하여 데이터셋을 구성합니다. \n","#이 과정은 각각의 뉴스그룹 샘플에 대해서 동일한 프로세스로 수행됩니다.\n","print('전체 샘플 수 :',len(skip_grams))"],"metadata":{"id":"3qGY-fE1mNxu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#encoded 중 상위 10개의 뉴스그룹 샘플에 대해서만 수행하였으므로 10이 출력됩니다. \n","#그리고 10개의 뉴스그룹 샘플 각각은 수많은 중심 단어, 주변 단어의 쌍으로 된 샘플들을 갖고 있습니다. \n","#첫번째 뉴스그룹 샘플이 가지고 있는 pairs와 labels의 개수를 출력해봅시다.\n","\n","# 첫번째 뉴스그룹 샘플에 대해서 생긴 pairs와 labels의 개수\n","print(len(pairs))\n","print(len(labels))\n"],"metadata":{"id":"hWxBycENmU3o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#이 작업을 모든 뉴스그룹 샘플에 대해서 수행합니다.\n","skip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded]"],"metadata":{"id":"FJHA4GbAmdGX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Skip-Gram with Negative Sampling(SGNS) 구현하기"],"metadata":{"id":"adI8uxdZmiPY"}},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Embedding, Reshape, Activation, Input\n","from tensorflow.keras.layers import Dot\n","from tensorflow.keras.utils import plot_model\n","from IPython.display import SVG\n","\n","embedding_dim = 100\n","\n","# 중심 단어를 위한 임베딩 테이블\n","w_inputs = Input(shape=(1, ), dtype='int32')\n","word_embedding = Embedding(vocab_size, embedding_dim)(w_inputs)\n","\n","# 주변 단어를 위한 임베딩 테이블\n","c_inputs = Input(shape=(1, ), dtype='int32')\n","context_embedding  = Embedding(vocab_size, embedding_dim)(c_inputs)"],"metadata":{"id":"RWqKYmojmhHO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#각 임베딩 테이블은 중심 단어와 주변 단어 각각을 위한 임베딩 테이블이며 각 단어는 임베딩 테이블을 거쳐서 내적을 수행하고, \n","#내적의 결과는 1 또는 0을 예측하기 위해서 시그모이드 함수를 활성화 함수로 거쳐 최종 예측값을 얻습니다.\n","dot_product = Dot(axes=2)([word_embedding, context_embedding])\n","dot_product = Reshape((1,), input_shape=(1, 1))(dot_product)\n","output = Activation('sigmoid')(dot_product)\n","\n","model = Model(inputs=[w_inputs, c_inputs], outputs=output)\n","model.summary()\n","model.compile(loss='binary_crossentropy', optimizer='adam')\n","plot_model(model, to_file='model3.png', show_shapes=True, show_layer_names=True, rankdir='TB')"],"metadata":{"id":"jh-92Ljfmnx-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for epoch in range(1, 6):\n","    loss = 0\n","    for _, elem in enumerate(skip_grams):\n","        first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n","        second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n","        labels = np.array(elem[1], dtype='int32')\n","        X = [first_elem, second_elem]\n","        Y = labels\n","        loss += model.train_on_batch(X,Y)  \n","    print('Epoch :',epoch, 'Loss :',loss)"],"metadata":{"id":"owe0LvEFmvo5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#결과확인\n","#학습된 임베딩 벡터들을 vector.txt에 저장합니다. \n","#그 후 이를 gensim의 models.KeyedVectors.load_word2vec_format()으로 로드하면 쉽게 단어 벡터 간 유사도를 구할 수 있습니다.\n","import gensim\n","\n","f = open('vectors.txt' ,'w')\n","f.write('{} {}\\n'.format(vocab_size-1, embed_size))\n","vectors = model.get_weights()[0]\n","for word, i in tokenizer.word_index.items():\n","    f.write('{} {}\\n'.format(word, ' '.join(map(str, list(vectors[i, :])))))\n","f.close()\n","\n","# 모델 로드\n","w2v = gensim.models.KeyedVectors.load_word2vec_format('./vectors.txt', binary=False)"],"metadata":{"id":"WITVw0JgmwlP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#예시\n","w2v.most_similar(positive=['soldiers'])\n","w2v.most_similar(positive=['doctor'])\n","w2v.most_similar(positive=['police'])\n","w2v.most_similar(positive=['knife'])\n","w2v.most_similar(positive=['engine'])"],"metadata":{"id":"7mwS2aMtm663"},"execution_count":null,"outputs":[]}]}