{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"노트북작업 11.17 Main.ipynb","private_outputs":true,"provenance":[],"mount_file_id":"1Ut04qPQgyn1pA5SdLAacMOcjrgrh4shj","authorship_tag":"ABX9TyOwp/DOJWcS65R9dZkrHYjF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"_nN3gGfi8jmQ"},"source":["#%%\n","import numpy as np\n","from tensorflow.keras import layers\n","from tensorflow.keras import Input\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras import regularizers\n","from tensorflow.keras import optimizers\n","import random\n","import tensorflow as tf\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.preprocessing import OneHotEncoder\n","# from sklearn.model_selection import train_test_splits\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","from sklearn.preprocessing import KBinsDiscretizer\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix\n","from collections import deque\n","import pandas as pd\n","\n","#%% 전처리하기\n","#train = pd.read_csv('C:/Users/LeeKihoon/PycharmProjects/gachonProject/Q-learning/Titanic/train.csv')\n","#test = pd.read_csv('C:/Users/LeeKihoon/PycharmProjects/gachonProject/Q-learning/Titanic/test.csv')\n","train = pd.read_csv('/content/drive/Othercomputers/310호 IIP/PycharmProjects/gachonProject/Q-learning/Titanic/train.csv')\n","test = pd.read_csv('/content/drive/Othercomputers/310호 IIP/PycharmProjects/gachonProject/Q-learning/Titanic/test.csv')\n","\n","labels = train['Survived'].values # labels == 생존했는지\n","\n","train01 = train #백업\n","\n","train01 = train01.drop([\"Survived\"], axis=1)\n","train01 = train01.drop([\"Name\"], axis=1)\n","train01 = train01.drop([\"PassengerId\"], axis=1)\n","train01 = train01.drop([\"Ticket\"], axis=1)\n","train01 = train01.drop([\"Cabin\"],axis=1)  #전처리귀찮은거 다버려\n"," \n","# 결측치 대체\n","train01['Age'] = train01['Age'].fillna(0)\n","train01['Embarked'] = train01['Embarked'].fillna('N')\n","#정수로 전처리\n","train01 = pd.get_dummies(data=train01, columns= ['Embarked'], prefix='Embarked')\n","train01 = pd.get_dummies(data=train01, columns= ['Sex'], prefix='Sex')\n","#2차 백업\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NPCO-GZF8lu6"},"source":["train_data = (train02 - train02.mean(axis=0)) / train02.std(axis=0)\n","train_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mNXqZUBEyyIy"},"source":["\n","\n","\"\"\" 이런식으로 변환해도 됌\n","from sklearn.preprocessing import LabelEncoder # 추가 추가 추가\n","train['Sex'] = LabelEncoder().fit_transform(train['Sex'])\n","test['Sex'] = LabelEncoder().fit_transform(test['Sex'])\n","\"\"\"\n","\n","\n","#%%\n","\"\"\" Setting \"\"\"\n","    # 전부 랜덤 시드 주기\n","random.seed(1)\n","np.random.seed(1)\n","tf.random.set_seed(1)\n","    # 강화학습에 필요한 값들 설정\n","DISCOUNT = 0.9 #누적보상할인계수\n","REPLAY_MEMORY_SIZE = 2000 #리플레이메모리 최대사이즈\n","MIN_REPLAY_MEMORY_SIZE = 512  # 256으로 설정가능. 최소사이즈\n","MIN_BATCH_SIZE = 128 #최소 배치사이즈\n","EPISODES = 100 #에피소드\n","epsilon = 0.9  # 앱실론 설정 (무작위탐험)\n","epsilon_decay = 0.99  # 0.8로도 설정가능. 앱실론에 곱해서 점점 줄여나감\n","MIN_EPSILON = 0.0001\n","BATCH_SIZE = 32 #배치사이즈\n","ALPHA = 1\n","ALPHA_DECAY = 1  # 1 0.9 # 0.9999 #0.9975\n","ALPHA_MIN = 0.0001\n","ssc = StandardScaler()   #평균을 제거하고 데이터를 단위 분산으로 조정\n","ssc = MinMaxScaler()     #모든 feature 값이 0~1사이에 있도록 데이터를 재조정\n","digitizer = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='kmeans') #10개의 구간으로 kmeans하게 나눔\n","\n","#%%\n","z = np.where(labels == 1, 100, -100)\n","\n","\n","#%%\n","class DQN():\n","    def __init__(self, input_size, output_size, data, labels):\n","        self.model = self.create_model(input_size, output_size)#기본모델\n","        self.target_model = self.create_model(input_size, output_size)#타겟모델\n","        self.obs = data #환경은 데이터 \n","        self.labels = labels #라벨 \n","        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n","        pass\n","\n","    def create_model(self, input_size, output_size):\n","\n","        input_tensor = Input(shape=(input_size,))\n","        x = layers.Dense(32, activation='relu')(input_tensor)\n","        x = layers.Dropout(0.1)(x)\n","        x = layers.Dense(32, activation='relu')(x)\n","        x = layers.Dense(16, activation='relu')(x)\n","        x = layers.Dropout(0.1)(x)\n","        x = layers.Dense(16, activation='relu')(x)\n","        x = layers.Dropout(0.1)(x)\n","        x = layers.Dense(8, activation='relu')(x)\n","        x = layers.Dropout(0.1)(x)\n","        output_tensor = layers.Dense(output_size, activation=None)(x)\n","        model = Model(inputs=input_tensor, outputs=output_tensor)\n","        # print (model.summary())\n","        ad = optimizers.Adam(learning_rate=0.001)  # 'rmsprop'\n","        model.compile(optimizer=ad,\n","                      loss='mse', metrics=['mae'])\n","        return model\n","\n","    def play(self, obs, action):\n","        r = np.where((self.obs == obs).all(axis=1)) #열에서 환경이 모두 같냐?\n","        print(\"r :\", r)\n","        if len(r[0]) == 0:\n","            # something really wrong observation not in the game\n","            print(\"........... SOMETHING WRONG ................\")\n","            return -100\n","        if self.labels[r[0][0]] == action:\n","            return 100\n","        else:\n","            return -100\n","\n","    def update_replay(self, obs, action, label, reward):\n","        self.replay_memory.append(((obs), action, label, reward))\n","\n","    def get_qs(self, obs):\n","        return self.model.predict(obs, batch_size=1)\n","\n","    def train(self):\n","        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\n","            return\n","        minibatch = random.sample(self.replay_memory, MIN_BATCH_SIZE)\n","        # conevrt minibatch in list type to y numpy array. This is needed for access\n","        # also keras and TF except in numpy format\n","        y = np.array([np.array(xi, dtype=np.dtype(object)) for xi in minibatch])\n","        # extract features as a numpy array. As needed by Keras.\n","        # shape is (batch_size, feature_size)\n","        z = np.array([np.array(xi) for xi in y[:, 0]])\n","        current_qa_list = dqn.model.predict(z, batch_size=BATCH_SIZE)\n","        future_qa_list = dqn.target_model.predict(z, batch_size=BATCH_SIZE)\n","        max_future_qa = np.argmax(future_qa_list, axis=1)\n","        alpha = ALPHA\n","        # qa = np.zeros(shape=(minibatch.shape[0],3))\n","        # Here only update the argmax because in theory we select max action\n","        # hence only the Q value of argmax get affected.\n","        # 이론상 최대 행동을 선택하기 때문에 argmax만 업데이트합니다.\n","        # 따라서 argmax의 Q값만 영향을 받습니다.\n","        for indx in range(y.shape[0]):\n","            # alpha1=1 gives 97 accuracy\n","            # current_qa_list[indx, y[indx, 1]] = (alpha1 * y[indx,3]) + ((1-alpha1)*max_future_qa[indx])\n","            # y[indx,1] is the action\n","            # y[indx,3] is the reward\n","            # z is the states / observation in numpy format\n","            # current_qa_list[indx, y[indx, 1]] = (alpha * y[indx,3]) + ((1-alpha)*max_future_qa[indx])\n","            current_qa_list[indx, y[indx, 1]] = current_qa_list[indx, y[indx, 1]] + (\n","                        alpha * (y[indx, 3]) - current_qa_list[indx, y[indx, 1]])\n","            pass\n","        dqn.model.fit(z, current_qa_list, epochs=16, batch_size=BATCH_SIZE, verbose=0)\n","        return\n","\n","    # dqn = DQN(train_data.shape[1], 2, train_data, labels)\n","\n","train_data, test_data, train_labels, test_labels = train_test_split(train_data, labels, test_size=0.3,random_state=True)\n","\n","#%%\n","\n","#%%\n","dqn = DQN(train_data.shape[1], 2, train_data, train_labels)\n","c = 0\n","total_rewards = 0\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i6r0w1l7-U1k"},"source":["zz = train_data[0:1].values.reshape(1,-1)\n","zz.shape\n","#train_data[indx].reshape(1, -1)) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rwXsdsV1DyKq"},"source":["train02[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aIf4_KqR-TMZ"},"source":["\n","#%%\n","while c < 1200:\n","    indx = np.random.randint(1, train_data.shape[0]) #총 623개 행중에 하나랜덤으로 고름\n","\n","    act = train_labels[indx] #액션은 랜덤으로 고른것. 0 or 1\n","    act = dqn.get_qs(train_data[indx-1:indx].values.reshape(1, -1)) \n","    print(\"act : \", act)\n","    if np.random.rand() > epsilon:\n","        act = np.argmax(act)\n","    else:\n","        act = np.random.randint(0, high=2)\n","\n","    r = dqn.play(train_data[indx-1:indx], action=act)\n","\n","    print(\"act2 : \", act)\n","    dqn.update_replay(train_data[indx-1 : indx], act, train_labels[indx-1 : indx], r)\n","    dqn.train()\n","    total_rewards = total_rewards + r\n","    if c%100 == 0 :\n","        print(c,\"번째 에피소드 reward : \", total_rewards)\n","    c = c + 1\n","    if ALPHA > ALPHA_MIN:\n","        ALPHA *= ALPHA_DECAY\n","    else:\n","        ALPHA = ALPHA_MIN\n","\n","    if epsilon > MIN_EPSILON:\n","        epsilon *= epsilon_decay\n","    else:\n","        epsilon = MIN_EPSILON\n","\n","#%%\n","results = np.argmax(dqn.model.predict(train_data, batch_size=BATCH_SIZE), axis=1)\n","a = accuracy_score(train_labels, results)\n","print(\" train accuracy \", a)\n","tn, fp, fn, tp = confusion_matrix(train_labels, results).ravel()\n","print(\" train tn ,\", tn, \" fp \", fp, \" fn \", fn, \" tp \", tp)\n","results = np.argmax(dqn.model.predict(test_data, batch_size=BATCH_SIZE), axis=1)\n","a = accuracy_score(test_labels, results)\n","print(\" Test accuracy \", a)\n","tn, fp, fn, tp = confusion_matrix(test_labels, results).ravel()\n","print(\" train tn ,\", tn, \" fp \", fp, \" fn \", fn, \" tp \", tp)"],"execution_count":null,"outputs":[]}]}