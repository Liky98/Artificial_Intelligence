{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'corpus/input_zh.txt' # 'corpus/text8.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "    \n",
    "def load(filepath, window_size, vocab_size=None):\n",
    "\n",
    "    words = []\n",
    "    with open(filepath, 'r', encoding='utf8') as file:\n",
    "        words = word_tokenize(file.readline())    \n",
    "\n",
    "    x_train, y_train = [], []\n",
    "    for i in range(len(words) - window_size + 1):\n",
    "        x_train.append(words[i: i + window_size - 1])\n",
    "        y_train.append(words[i +  window_size - 1])\n",
    "    \n",
    "    vocab = [word[0] for word in Counter(words).most_common(vocab_size)]\n",
    "    word2id = { vocab[i]: i for i in range(len(vocab)) }\n",
    "    \n",
    "    return np.array(x_train), np.array(y_train)[:,None], np.array(vocab), word2id\n",
    "\n",
    "def load_zh(filepath, window_size, vocab_size=None):\n",
    "\n",
    "    words = []\n",
    "    with open(filepath, 'r', encoding='utf8') as file:\n",
    "        for line in file:\n",
    "            words += word_tokenize(line.strip())\n",
    "        \n",
    "\n",
    "    x_train, y_train = [], []\n",
    "    for i in range(len(words) - window_size + 1):\n",
    "        x_train.append(words[i: i + window_size - 1])\n",
    "        y_train.append(words[i +  window_size - 1])\n",
    "    \n",
    "    vocab = [word[0] for word in Counter(words).most_common(vocab_size)]\n",
    "    word2id = { vocab[i]: i for i in range(len(vocab)) }\n",
    "    \n",
    "    return np.array(x_train), np.array(y_train)[:,None], np.array(vocab), word2id\n",
    "            \n",
    "def convert_to_id(x_train, y_train, vocab):\n",
    "    \n",
    "    word_to_id = {}\n",
    "    for i, vocab in enumerate(vocab):\n",
    "        word_to_id[vocab] = i\n",
    "        \n",
    "    for i in range(len(x_train)):\n",
    "        x_train[i] = [word_to_id[word] for word in x_train[i]]\n",
    "        y_train[i] = word_to_id[y_train[i][0]]\n",
    "        \n",
    "    return x_train.astype(int), y_train.astype(int)\n",
    "\n",
    "\n",
    "def next_batch(x_train, y_train, batch_size):\n",
    "    \n",
    "    num_batch = len(x_train) // batch_size + 1\n",
    "    for n in range(num_batch):        \n",
    "        offset = n * batch_size\n",
    "        x_batch = x_train[offset: offset + batch_size]\n",
    "        y_batch = y_train[offset: offset + batch_size]\n",
    "        \n",
    "        yield x_batch, y_batch\n",
    "        \n",
    "# def convert_to_word(x_train, y_train, id_to_word):\n",
    "#     for i in range(len(x_train)):\n",
    "#         print(x_train[i])\n",
    "#         x_train[i] = id_to_word[x_train[i]]\n",
    "#         y_train[i] = id_to_word[y_train[i]]\n",
    "#     return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter\n",
    "batch_size = 32\n",
    "window_size = 5\n",
    "vocab_size = None\n",
    "hidden_size = 10\n",
    "emb_dim = 50\n",
    "learning_rate = 0.8\n",
    "epoch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_raw, y_raw, vocab, word2id = load_zh(filepath, window_size, vocab_size)\n",
    "vocab_size = len(vocab)\n",
    "print('vocab_size: {}'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = convert_to_id(x_raw, y_raw, vocab)\n",
    "print('Length: {}'.format(len(x_train)))\n",
    "print('Number of batch: {}'.format(len(x_train) / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameter Definition\n",
    "\n",
    "\n",
    "# Input && Output\n",
    "input_words = tf.placeholder(dtype=tf.int32, shape=(batch_size, window_size-1))\n",
    "output_word = tf.placeholder(dtype=tf.int32, shape=(batch_size, 1))\n",
    "\n",
    "\n",
    "# Word Features\n",
    "C = tf.Variable(tf.truncated_normal(shape=(vocab_size, emb_dim), mean=-1, stddev=-1), name='word_embedding')\n",
    "\n",
    "\n",
    "# Hidden Layer Weight && Bias\n",
    "H = tf.Variable(tf.random_normal(shape=(hidden_size, (window_size - 1 ) * emb_dim)))\n",
    "d = tf.Variable(tf.random_normal(shape=(hidden_size, )))\n",
    "\n",
    "# Hidden-to-Output Weight && Bias\n",
    "U = tf.Variable(tf.random_normal(shape=(vocab_size, hidden_size)))\n",
    "b = tf.Variable(tf.random_normal(shape=(vocab_size, )))\n",
    "\n",
    "# Projection-to-Output Weight\n",
    "W = tf.Variable(tf.random_normal(shape=(vocab_size, (window_size - 1) * emb_dim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = b + Wx + Utanh(d + Hx)\n",
    "\n",
    "# x = (C(w(t-1)), C(w(t-2), ..., C(w(t-n+1))), n == window_size\n",
    "with tf.name_scope('Projection_Layer'):\n",
    "    x  = tf.nn.embedding_lookup(C, input_words) # (batch_size, window_size-1, emb_dim)\n",
    "    x  = tf.reshape(x, shape=(batch_size, (window_size - 1) * emb_dim))\n",
    "    \n",
    "with tf.name_scope('Hidden_Layer'):\n",
    "    Hx = tf.matmul(x, tf.transpose(H)) # (batch_size, hidden_size)\n",
    "    o  = tf.add(d, Hx) # (batch_size, hidden_size)\n",
    "    a  = tf.nn.tanh(o)  # (batch_size, hidden_size)\n",
    "     \n",
    "with tf.name_scope('Output_Layer'):\n",
    "    Ua = tf.matmul(a, tf.transpose(U)) # (batch_size, vocab_size)\n",
    "    Wx = tf.matmul(x, tf.transpose(W)) # (batch_size, vocab_size)\n",
    "    y  = tf.nn.softmax(tf.clip_by_value(tf.add(b, tf.add(Wx, Ua)), 0.0, 10)) # (batch_size, vocab_size)\n",
    "    \n",
    "\n",
    "with tf.name_scope('Loss'):\n",
    "    onehot_tgt = tf.one_hot(tf.squeeze(output_word), vocab_size)  # (batch_size, vocab_size)\n",
    "    loss = -1 * tf.reduce_mean(tf.reduce_sum(tf.log(y) * onehot_tgt, 1)) # 乘 -1 -> maximize loss\n",
    "    print(loss)\n",
    "    \n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=True)) as sess:\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    initializer.run()\n",
    "    \n",
    "    step = 0\n",
    "    avg_loss = 0\n",
    "    for epoch in range(epoch_size):\n",
    "        for x_batch, y_batch in next_batch(x_train, y_train, batch_size):\n",
    "            if len(x_batch) != batch_size:\n",
    "                continue\n",
    "            feed_dict = {input_words: x_batch, output_word: y_batch}\n",
    "            fetches = [loss, optimizer]\n",
    "            Loss, _ = sess.run(fetches, feed_dict)\n",
    "            avg_loss += Loss\n",
    "            if step % 100 == 0:\n",
    "                print('Step {}, Loss: {}'.format(step, avg_loss / 100))\n",
    "                avg_loss = 0\n",
    "            \n",
    "            step += 1\n",
    "    \n",
    "    print('Training Done.')\n",
    "    word_embedding = C.eval()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "def cosine_similarity(wordvec1, wordvec2):\n",
    "    return np.dot(wordvec1, wordvec2) / (np.linalg.norm(wordvec1) * np.linalg.norm(wordvec2))\n",
    "\n",
    "def distance(wordvec1, wordvec2):\n",
    "    return (np.linalg.norm(wordvec1 - wordvec2))\n",
    "    \n",
    "def most_similar(word_embedding, word):\n",
    "    \n",
    "    target = word2id[word]\n",
    "    n = 10\n",
    "    top10 = [('', 100000) for i in range(n)]\n",
    "    \n",
    "    \n",
    "    for i in range(len(word_embedding)):\n",
    "        if i == target:\n",
    "            continue\n",
    "        \n",
    "        sim = distance(word_embedding[target], word_embedding[i])\n",
    "        for j in range(n):\n",
    "            if sim <= top10[j][1]:\n",
    "                top10[j+1:] = top10[j:-1]\n",
    "                top10[j] = (vocab[i], sim)\n",
    "                break        \n",
    "    pprint(top10)\n",
    "\n",
    "most_similar(word_embedding, '九月')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "wordemb_2D = pca.fit_transform(word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "x_coords = wordemb_2D[:n, 0]\n",
    "y_coords = wordemb_2D[:n, 1]\n",
    "\n",
    "plt.scatter(x_coords, y_coords, c='b')\n",
    "for label, x, y in zip(vocab[:n], x_coords, y_coords):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "sentences = word2vec.LineSentence(filepath)\n",
    "model = word2vec.Word2Vec(sentences, size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive='九月')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
